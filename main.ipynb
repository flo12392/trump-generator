{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model\n",
    "import keras.utils\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "from src.read_data import read_trump_speeches\n",
    "from src.utils import print_tensorflow_devices\n",
    "from src.data_generator import DataGenerator\n",
    "print_tensorflow_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "seq_len = 21 # includes next word.\n",
    "\n",
    "# NN parameters\n",
    "batch_size = 128\n",
    "\n",
    "examples_file_loc = 'examples/examples.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speeches = read_trump_speeches('data/speeches.txt')\n",
    "words = np.unique(speeches)\n",
    "word_index = dict((c, i) for i, c in enumerate(words))\n",
    "index_word = dict((i, c) for i, c in enumerate(words))\n",
    "n_words = len(words)\n",
    "\n",
    "speeches_indexed = [word_index[x] for x in speeches]\n",
    "sentence_ranges = [range(i,i+seq_len) for i in range(0,len(speeches)-seq_len)]\n",
    "sentences = [[speeches[y] for y in x] for x in sentence_ranges]\n",
    "sentences_indexed = [[speeches_indexed[y] for y in x] for x in sentence_ranges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# Function modified from https://github.com/enriqueav/lstm_lyrics/blob/master/lstm_train.py\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    sentence = (sentences_indexed_test)[np.random.randint(len(sentences_indexed_test))]\n",
    "\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        examples_file.write('\\n----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join([index_word[x] for x in sentence]) + '\"\\n')\n",
    "\n",
    "        full_sentence = sentence.copy()\n",
    "        \n",
    "        for i in range(50): \n",
    "            sentence = sentence[1:]\n",
    "            x_pred = np.zeros((1, seq_len - 1, n_words), dtype=np.bool)\n",
    "            for t, w in enumerate(sentence):\n",
    "                x_pred[0, t, w] = 1\n",
    "                \n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            sentence.append(next_index)\n",
    "            full_sentence.append(next_index)\n",
    "            \n",
    "        full_sentence = [index_word[x] for x in full_sentence]\n",
    "        examples_file.write(' '.join(full_sentence))\n",
    "    examples_file.write('\\n' + '='*80 + '\\n\\n')\n",
    "    examples_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "random.shuffle(sentences_indexed)\n",
    "train_split = int(0.95*len(sentences_indexed))\n",
    "sentences_indexed_train = sentences_indexed[:train_split]\n",
    "sentences_indexed_test = sentences_indexed[train_split:]\n",
    "print('Train: ' + str(len(sentences_indexed_train)))\n",
    "print('Test: ' +str(len(sentences_indexed_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg = DataGenerator(sentences_indexed_train[0:1], seq_len, n_words, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(256, activation=\"relu\"),input_shape=(seq_len-1, n_words)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_words, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_be_loaded = None\n",
    "if model_to_be_loaded is not None:\n",
    "    model = load_model('models/' + model_to_be_loaded)\n",
    "else:\n",
    "    model = get_model()\n",
    "\n",
    "optimizer = Adam(lr=0.002)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "callbacks_list = [print_callback, early_stopping]\n",
    "examples_file = open(examples_file_loc, \"w\")\n",
    "\n",
    "model.fit_generator(DataGenerator(sentences_indexed_train, seq_len, n_words, batch_size),\n",
    "                    steps_per_epoch=int(len(sentences_indexed_train)/batch_size) + 1,\n",
    "                    epochs=100,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=DataGenerator(sentences_indexed_test, seq_len, n_words, batch_size),\n",
    "                    validation_steps=int(len(sentences_indexed_test)/batch_size) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('models/' + time.strftime(\"%Y%m%d-%H%M%S\") + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_model = '20180831-163242.h5'\n",
    "model = load_model('models/' + prediction_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('\\n ------------------------------------------------------------------- \\n\\n')\n",
    "        # Randomly pick a seed sequence\n",
    "    seed = (sentences_indexed)[np.random.randint(len(sentences_indexed))]\n",
    "    sentence = seed.copy()\n",
    "\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "\n",
    "        full_sentence = sentence.copy()\n",
    "\n",
    "        for i in range(250): \n",
    "            sentence = sentence[1:]\n",
    "            x_pred = np.zeros((1, seq_len - 1, n_words), dtype=np.bool)\n",
    "            for t, w in enumerate(sentence):\n",
    "                x_pred[0, t, w] = 1\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            sentence.append(next_index)\n",
    "            full_sentence.append(next_index)\n",
    "\n",
    "        full_sentence = [index_word[x] for x in full_sentence]\n",
    "        full_sentence = ' '.join(full_sentence)\n",
    "        print(full_sentence)\n",
    "        print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
