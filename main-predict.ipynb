{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import random\n",
    "from src.read_data import read_trump_speeches\n",
    "from src.utils import print_tensorflow_devices\n",
    "from src.utils import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "seq_len = 21 # includes next word.\n",
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = read_trump_speeches('data/speeches.txt')\n",
    "words = np.unique(speeches)\n",
    "word_index = dict((c, i) for i, c in enumerate(words))\n",
    "index_word = dict((i, c) for i, c in enumerate(words))\n",
    "n_words = len(words)\n",
    "\n",
    "# Index the speeches, then create sentences of length 'seq_len' that we can train the RNN on.\n",
    "speeches_indexed = [word_index[x] for x in speeches]\n",
    "sentence_ranges = [range(i, i + seq_len) for i in range(0, len(speeches) - seq_len)]\n",
    "sentences = [[speeches[y] for y in x] for x in sentence_ranges]\n",
    "sentences_indexed = [[speeches_indexed[y] for y in x] for x in sentence_ranges]\n",
    "\n",
    "# Train-test split\n",
    "random.seed(12345)\n",
    "random.shuffle(sentences_indexed)\n",
    "train_split = int(0.95 * len(sentences_indexed))\n",
    "sentences_indexed_train = sentences_indexed[:train_split]\n",
    "sentences_indexed_test = sentences_indexed[train_split:]\n",
    "print('Train: ' + str(len(sentences_indexed_train)))\n",
    "print('Test: ' + str(len(sentences_indexed_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model = '20180902-040431.h5'\n",
    "model = load_model('models/' + prediction_model)\n",
    "\n",
    "examples_file = open('examples/' + prediction_model + '.txt', 'w')\n",
    "\n",
    "for i in range(10):\n",
    "    # Randomly pick a seed sequence\n",
    "    seed = (sentences_indexed)[np.random.randint(len(sentences_indexed))]\n",
    "    sentence = seed.copy()\n",
    "    examples_file.write( '----- Generating with seed: \"' + ' '.join([index_word[x] for x in seed]) + '\"\\n\\n')\n",
    "\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        \n",
    "        examples_file.write('Diversity: ' + str(diversity) + '\\n')\n",
    "        sentence = seed\n",
    "        full_sentence = sentence.copy()\n",
    "\n",
    "        for i in range(250):\n",
    "            sentence = sentence[1:]\n",
    "            x_pred = np.zeros((1, seq_len - 1, n_words), dtype=np.bool)\n",
    "            for t, w in enumerate(sentence):\n",
    "                x_pred[0, t, w] = 1\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            sentence.append(next_index)\n",
    "            full_sentence.append(next_index)\n",
    "\n",
    "        full_sentence = [index_word[x] for x in full_sentence]\n",
    "        full_sentence = ' '.join(full_sentence)\n",
    "        examples_file.write(full_sentence)\n",
    "        examples_file.write('\\n\\n')\n",
    "    examples_file.write('\\n\\n')\n",
    "\n",
    "examples_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
